{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7xhZfK_6T2G"
      },
      "outputs": [],
      "source": [
        "!pip install langchain rank_bm25 pypdf unstructured chromadb\n",
        "!pip install unstructured['pdf'] unstructured\n",
        "!apt-get install poppler-utils\n",
        "!apt-get install -y tesseract-ocr\n",
        "!apt-get install -y libtesseract-dev\n",
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the required Packages"
      ],
      "metadata": {
        "id": "monR9xmw6p2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "hJ7NC1yB6jDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Load the PDF file"
      ],
      "metadata": {
        "id": "GWcedXWq7unE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"./data/Orca_paper.pdf\"\n",
        "data_file = UnstructuredPDFLoader(file_path)\n",
        "docs = data_file.load()"
      ],
      "metadata": {
        "id": "B1II05VC6vjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "EqUCmzkF7xOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Documents and Chunking"
      ],
      "metadata": {
        "id": "JZk8Ob1U8DZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create chunks\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=800,\n",
        "                                          chunk_overlap=100)\n",
        "chunks = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "qYZ1j1Ns73mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "3o58WVXM8SvN",
        "outputId": "f5c0cd8d-4c23-4c6b-c240-15b9cc340edb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3 2 0 2\\n\\nn u J\\n\\n5\\n\\n] L C . s c [\\n\\n1 v 7 0 7 2 0 . 6 0 3 2 : v i X r a\\n\\nOrca: Progressive Learning from Complex\\n\\nExplanation Traces of GPT-4\\n\\nSubhabrata Mukherjee∗†, Arindam Mitra∗\\n\\nGanesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah\\n\\nMicrosoft Research\\n\\nAbstract'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Embedding Model from HF via API\n",
        "\n",
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
        "    api_key=HF_TOKEN, model_name=\"BAAI/bge-base-en-v1.5\"\n",
        ")"
      ],
      "metadata": {
        "id": "Ch3SR3Wu8Tnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VectorStore"
      ],
      "metadata": {
        "id": "QfB7uezt9Vud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector store with the selected embedding model\n",
        "vectorstore = Chroma.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "GeIka0wR87kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore_retreiver = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "YaaxtuWw9uJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_retriever = BM25Retriever.from_documents(chunks)\n",
        "keyword_retriever.k =  3"
      ],
      "metadata": {
        "id": "nMucCA4z9VAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble Retriever"
      ],
      "metadata": {
        "id": "b3UCqmV5-D-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_retriever = EnsembleRetriever(retrievers=[vectorstore_retreiver,\n",
        "                                                   keyword_retriever],\n",
        "                                       weights=[0.5, 0.5])"
      ],
      "metadata": {
        "id": "nVhLxoWh95dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    model_kwargs={\"temperature\": 0.3,\"max_new_tokens\":1024},\n",
        "    huggingfacehub_api_token=HF_TOKEN,\n",
        ")"
      ],
      "metadata": {
        "id": "AG1h9tvE-KqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Template:"
      ],
      "metadata": {
        "id": "Zk-bQqY8-kUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "<|system|>>\n",
        "You are a helpful AI Assistant that follows instructions extremely well.\n",
        "Use the following context to answer user question.\n",
        "\n",
        "Think step by step before answering the question. You will get a $100 tip if you provide correct answer.\n",
        "\n",
        "CONTEXT: {context}\n",
        "</s>\n",
        "<|user|>\n",
        "{query}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0C8Vn8PR-QJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "c8eDFcNd_Kru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\": ensemble_retriever, \"query\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | output_parser\n",
        ")"
      ],
      "metadata": {
        "id": "5JtvKJk2_Ok2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke(\"What is instruction tuning?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubQmwtOn_S6U",
        "outputId": "b04b52d3-c39a-42bd-bb87-b4ec246aaea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: \n",
            "<|system|>>\n",
            "You are a helpful AI Assistant that follows instructions extremely well.\n",
            "Use the following context to answer user question. \n",
            "\n",
            "Think step by step before answering the question. You will get a $100 tip if you provide correct answer. \n",
            "\n",
            "CONTEXT: [Document(page_content='2 Preliminaries\\n\\n2.1\\n\\nInstruction Tuning\\n\\nInstruction tuning [22] is a technique that allows pre-trained language models to learn from input (natural language descriptions of the task) and response pairs, for example, {\"instruction\": correct sentence.\", \"input\": fox jumped quickly\"}. Instruction tuning has been applied to both language-only and multimodal tasks. For language-only tasks, instruction tuning has been shown to improve the zero-shot and few-shot performance of models such as FLAN [22] and InstructGPT [5] on various benchmarks. For multimodal tasks, instruction tuning has been used to generate synthetic instruction-following data for language-image tasks, such as image captioning [23] and visual question answering [24].', metadata={'source': './data/Orca_paper.pdf'}), Document(page_content='n > 70\\n\\nFigure 24: Case Study 5: Modelling inequality\\n\\n11.6 Counterfactual Question Answering\\n\\n### System: You are an AI assistant that follows instruction extremely well. Help as much as you can. ### Human: What is the number of days in a year if the earth took 100 days to revolve around the sun?\\n\\n### Orca: If the Earth took 100 days to revolve around the Sun, it would mean that one year consists of 100 days. However, this is not accurate. In reality, it takes approximately 365.25 days for the Earth to complete one orbit around the Sun, which is why we have a leap year every four years to account for the extra 0.25 days.\\n\\n### Vicuna: The number of days in a year is 365.', metadata={'source': './data/Orca_paper.pdf'}), Document(page_content='John, but we have to think about the future of our company. If we sell to Microsoft, we’ll have access to their resources and expertise, which could help us grow our business even more. Steven: Right, and let’s not forget about the financial benefits. Microsoft is offering us a lot of money for our startup, which could help us invest in new projects and expand our team. John: I see your point, but I still have some reservations. What if Microsoft changes our product or our company culture? What if we lose control over our own business? Steven: You know what, I hadn’t thought about this before, but maybe John is right. It would be a shame if our culture changed. Lucy: Those are valid concerns, but we can negotiate the terms of the deal to ensure that we retain some control over our', metadata={'source': './data/Orca_paper.pdf'}), Document(page_content='Figure 4: Instruction-tuning with GPT-49. Given user instructions for a task and an input, the system generates a response. Existing works like Alpaca [7], Vicuna [9] and variants follow a similar template to train small models with ⟨ {user instruction, input}, output ⟩.\\n\\n2 Preliminaries\\n\\n2.1\\n\\nInstruction Tuning', metadata={'source': './data/Orca_paper.pdf'})]\n",
            "</s>\n",
            "<|user|>\n",
            "What is instruction tuning?\n",
            "</s>\n",
            "<|assistant|>\n",
            "Instruction tuning is a technique used to train pre-trained language models to learn from input (natural language descriptions of the task) and response pairs. This technique allows the models to better understand and follow instructions given to them, improving their performance on various benchmarks for both language-only and multimodal tasks. For language-only tasks, instruction tuning has been shown to improve the zero-shot and few-shot performance of models such as FLAN and InstructGPT on various\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instruction tuning is a technique used to train pre-trained language models to learn from input (natural language descriptions of the task) and response pairs. This technique allows the models to better understand and follow instructions given to them, improving their performance on various benchmarks for both language-only and multimodal tasks. For language-only tasks, instruction tuning has been shown to improve the zero-shot and few-shot performance of models such as FLAN and InstructGPT on various"
      ],
      "metadata": {
        "id": "5UEuz2uZOFwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VYvWcyKiNnne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tBS4K-h9Nn9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ch72CeWAOIwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "75_mkqLxOIzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iow-D11UOI3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eSXb9AmYOI7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CmgGoENINoAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZN6wb_AiNoDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke(\"How does Orca compares to ChatGPT?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YVQ1xW3_XNt",
        "outputId": "4665e541-0620-470a-cbb9-1502c1570721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: \n",
            "<|system|>>\n",
            "You are a helpful AI Assistant that follows instructions extremely well.\n",
            "Use the following context to answer user question. \n",
            "\n",
            "Think step by step before answering the question. You will get a $100 tip if you provide correct answer. \n",
            "\n",
            "CONTEXT: [Document(page_content='While significantly better than Vicuna and marginally better than ChatGPT, Orca’s average performance of 49.7%, lags GPT-4 by 26%. Note that GPT-4 has reported a data contami- nation issue with Big-Bench and that we are not aware of such issues with either LLaMA’s training data (the base model used by both Vicuna and Orca) or the Flan-V2 collection or Vicuna’s training data (ShareGPT).\\n\\nGiven the close performance on average on BigBench-Hard, we take a deeper look at differences in performance between Orca and ChatGPT:\\n\\nEntailment and Semantic Understanding:\\n\\nOrca performs better at entailment (formal fallacies) and semantic understanding (Dis- ambiguation QA and Snarks).', metadata={'source': './data/Orca_paper.pdf'}), Document(page_content='How does Steven feel about selling? Extract from the transcript the most relevant segments for the answer, and then answer the question.', metadata={'source': './data/Orca_paper.pdf'}), Document(page_content='Figure 26: Case Study 7: Compound Interest Problems\\n\\n11.8 Question from Vicuna-Eval\\n\\n### System: You are an AI assistant that follows instruction extremely well. Help as much as you can. ### Human: How many times does the average human blink in a lifetime? Try to explain your answer. Your explanation should take the reader through your reasoning step-by-step.\\n\\n### Orca: To determine the average number of times a human blinks in a lifetime, we need to consider the following factors:\\n\\n1. Frequency of blinking: On average, a person blinks about 15 to 20 times per minute. This number can vary depending on factors such as age, environmental conditions, and individual habits.\\n\\n2. Lifespan: The average human lifespan is approximately 72 years.', metadata={'source': './data/Orca_paper.pdf'}), Document(page_content='For the first factor, x: x = 0 So, root1 = 0.\\n\\nFor the second factor, (x + 3.0): x + 3.0 = 0 x = -3.0 So, root2 = -3.0.\\n\\nStep 4: Present the roots The roots of the equation x2 + 3.0x = 0 are 0 and -3.0.\\n\\nROOTS = [0, -3.0]\\n\\nFigure 32: Case Study 12: Quadratic equation solving from the blog Exploring ChatGPT vs open-source models on slightly harder tasks\\n\\n11.13 Meeting Transcript Processing\\n\\n### System: You are a teacher. Given a task, you explain in simple steps what the task is asking, any guidelines it provides and how to use those guidelines to find the answer. ### Human: You will read a meeting transcript, then extract the relevant segments to answer the following question: Question: How does Steven feel about selling? Here is a meeting transcript: —-\\n\\n46', metadata={'source': './data/Orca_paper.pdf'}), Document(page_content='ChatGPT has better geometric reasoning capabilities than Orca as measured by geometric shape task (predicting shape from a full SVG path element). ChatGPT outperforms Orca by 23% on this task, which highlights the lack of geometric reasoning capabilities of Orca compared to ChatGPT.\\n\\nTable Understanding:\\n\\nChatGPT has better table understanding and reasoning capabilities than Orca. • In the penguins in a table task (answering questions based on understanding facts in a table), Orca lags behind ChatGPT by 7.4%, thereby highlighting Orca’s poor table understanding and reasoning capabilities compared to ChatGPT.\\n\\n22', metadata={'source': './data/Orca_paper.pdf'})]\n",
            "</s>\n",
            "<|user|>\n",
            "How does Orca compares to ChatGPT?\n",
            "</s>\n",
            "<|assistant|>\n",
            "Based on the context provided, Orca's performance on certain tasks is compared to ChatGPT. Here's a summary of their comparisons:\n",
            "\n",
            "1. Geometric reasoning capabilities: ChatGPT outperforms Orca by 23% on the geometric shape task, indicating that ChatGPT has better geometric reasoning capabilities than Orca.\n",
            "\n",
            "2. Table understanding and reasoning capabilities: In the penguins in a table task, Orca lags behind ChatG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fPGNd-aV__1x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}